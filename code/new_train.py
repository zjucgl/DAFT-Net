import matplotlib
import pandas as pd
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torch import nn, optim
from torch.utils.data import DataLoader
from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score  # æ–°å¢è¯„ä»·æŒ‡æ ‡

from data_processing import preprocess_data
from dataset import MyDataset
from models.twoStage_aft_model import FusionAFTModel
from models.twoStage_mlpmixer_model import FusionMLPMixerModel
from models.twoStage_update_aft_model import FusionAFTUPModel

# ==================== 1ï¸âƒ£ è¯»å–æ•°æ® ====================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
matplotlib.rc("font",family='WenQuanYi Micro Hei')

# è®­ç»ƒé›†
data = pd.read_csv('data/20240205133204/train.csv')
data = data.applymap(lambda x: x.strip() if isinstance(x, str) else x)  # æ¸…ç†æ•°æ®
data = preprocess_data(data)
dataset = MyDataset(data)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# æµ‹è¯•é›†
test_data = pd.read_csv('data/20240205133204/test.csv')
test_data = test_data.applymap(lambda x: x.strip() if isinstance(x, str) else x)
test_data = preprocess_data(test_data)
test_dataset = MyDataset(test_data)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# ==================== 2ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹ ====================
# model = FusionAFTModel().to(device)
# model = FusionMLPMixerModel().to(device)
model = FusionAFTUPModel().to(device)
criterion = nn.BCELoss()  # äºŒå…ƒäº¤å‰ç†µæŸå¤±
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®°å½•æœ€ä½³ AUC
best_auc = 0.0
best_model_path = "best_model_auc.pth"

# ==================== 3ï¸âƒ£ è®­ç»ƒ + æµ‹è¯•éªŒè¯ ====================
num_epochs = 20  # è®­ç»ƒè½®æ•°

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    all_train_labels, all_train_preds = [], []

    for batch in dataloader:
        basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded, labels = batch
        basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded, labels = (
            basic_data.to(device), blood_data.to(device), text_data_encoded.to(device),
            raw_data.to(device), basic_risk_data_encoded.to(device), blood_risk_data_encoded.to(device),
            labels.to(device).float()
        )

        optimizer.zero_grad()
        disease_pred, _ = model(basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded)

        loss = criterion(disease_pred.squeeze(), labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # è®°å½•è®­ç»ƒé›†çœŸå®å€¼å’Œé¢„æµ‹å€¼
        all_train_labels.extend(labels.cpu().numpy())
        all_train_preds.extend(disease_pred.squeeze().cpu().detach().numpy())

    # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
    avg_loss = total_loss / len(dataloader)
    train_mse = mean_squared_error(all_train_labels, all_train_preds)
    train_acc = accuracy_score(all_train_labels, np.round(all_train_preds))
    train_auc = roc_auc_score(all_train_labels, all_train_preds)

    # ==================== 4ï¸âƒ£ åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯ ====================
    model.eval()
    all_test_labels, all_test_preds = [], []

    with torch.no_grad():
        for batch in test_dataloader:
            basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded, labels = batch
            basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded = (
                basic_data.to(device), blood_data.to(device), text_data_encoded.to(device),
                raw_data.to(device), basic_risk_data_encoded.to(device), blood_risk_data_encoded.to(device)
            )

            disease_pred, _ = model(basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded)

            all_test_labels.extend(labels.numpy())
            all_test_preds.extend(disease_pred.squeeze().cpu().numpy())

    # è®¡ç®—æµ‹è¯•é›†æŒ‡æ ‡
    test_mse = mean_squared_error(all_test_labels, all_test_preds)
    test_acc = accuracy_score(all_test_labels, np.round(all_test_preds))
    test_auc = roc_auc_score(all_test_labels, all_test_preds)

    print(f"Epoch [{epoch+1}/{num_epochs}]")
    print(f"Train  - Loss: {avg_loss:.4f}, MSE: {train_mse:.4f}, ACC: {train_acc:.4f}, AUC: {train_auc:.4f}")
    print(f"Test   - MSE: {test_mse:.4f}, ACC: {test_acc:.4f}, AUC: {test_auc:.4f}")

    # å¦‚æœæµ‹è¯•é›† AUC æ›´é«˜ï¼Œåˆ™ä¿å­˜æ¨¡å‹
    if test_auc > best_auc:
        best_auc = test_auc
        torch.save(model.state_dict(), best_model_path)
        print(f"ğŸ‰ New best model saved with AUC: {best_auc:.4f}")

# åŠ è½½ AUC æœ€é«˜çš„æ¨¡å‹
model.load_state_dict(torch.load(best_model_path))
model.eval()
print(f"âœ… Loaded best model with AUC: {best_auc:.4f}")

# ==================== 5ï¸âƒ£ è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ç‰¹å¾é‡è¦æ€§ ====================
all_importance_scores = []

with torch.no_grad():
    for batch in dataloader:
        basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded, labels = batch
        basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded = (
            basic_data.to(device), blood_data.to(device), text_data_encoded.to(device),
            raw_data.to(device), basic_risk_data_encoded.to(device), blood_risk_data_encoded.to(device)
        )

        _, importance_scores = model(basic_data, blood_data, text_data_encoded, raw_data, basic_risk_data_encoded, blood_risk_data_encoded)
        all_importance_scores.append(importance_scores.cpu().numpy())

# è®¡ç®—ç‰¹å¾é‡è¦æ€§å‡å€¼
all_importance_scores_np = np.mean(np.vstack(all_importance_scores), axis=0)

# å®šä¹‰ç‰¹å¾åç§°
# feature_names = ["è¶…å£°å¿ƒç”µå›¾ç­‰", "åŸå§‹æ•°æ®","è¡€è„‚ä¿¡æ¯", "WHOé£é™©è¯„ä¼°", "ASCVDé£é™©è¯„ä¼°"]
feature_names = ["è¶…å£°å¿ƒç”µå›¾CTçš„æ–‡æœ¬è¯Šæ–­ç­‰",
                 'gender', 'age', 'is_diabetes', 'is_hypertension',
                 'is_smoking', 'years_of_smoking', 'is_drinking', 'is_family_history',
                 'heart_rate','respiratory_rate', 'diastolic_pressure','systolic_pressure',
                 'body_mass_index',
                 "TC","TG","HDL-C", 'LDL-C', 'LPa', 'apoAI', 'apoB']

# ==================== 6ï¸âƒ£ å¯è§†åŒ– ====================

# (1) ç»˜åˆ¶æŸ±çŠ¶å›¾
plt.figure(figsize=(10, 5))
sns.barplot(x=feature_names, y=all_importance_scores_np, palette="Blues_r")
plt.xlabel("Feature Categories")
plt.ylabel("Feature Importance Score")
plt.title("Feature Importance (Attention-based)")
plt.xticks(rotation=30)
plt.show()

# (2) ç»˜åˆ¶é›·è¾¾å›¾
def plot_radar_chart(values, labels, title="Feature Importance Radar Chart"):
    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
    values = values.tolist()
    values += values[:1]
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    ax.fill(angles, values, color='blue', alpha=0.3)
    ax.plot(angles, values, color='blue', linewidth=2)
    ax.set_yticklabels([])
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels)
    plt.title(title)
    plt.show()

plot_radar_chart(all_importance_scores_np, feature_names)
